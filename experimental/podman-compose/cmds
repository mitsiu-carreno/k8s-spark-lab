podman build -t cluster-apache-spark:3.5.3 .

podman-compose up

podman exec -it spark-master ./bin/spark-submit --master spark://spark-master:7077 --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 --conf spark.hadoop.fs.s3a.access.key=accesskey --conf spark.hadoop.fs.s3a.secret.key=secretkey --conf spark.hadoop.fs.s3a.path.style.access=true --packages org.apache.hadoop:hadoop-aws:3.3.4 /opt/spark-apps/test.py


scp apps/*  designar4@192.168.100.20:/home/designar4/spark-cluster/compose/apps/

ssh -L :12345:localhost:4040 designar4@192.168.100.20


# Start new named screen session
screen -S s3-cp

# Detach from named session
Ctrl + A, then D

# Can even break the ssh connection

# Reattach to session
screen -r s3-cp

# Development
Formatting
black file.py 

Lintting
flake8 file.py
pylint file.py

# Snippets
```
row = df.take(1)
print(row)
```

```
df.show(truncate=False)
print(f"{df.count()}, {len(df.columns)}")

df.printSchema()
```

```
rows = df.limit(10020).collect()[10000:10020]
df_subset = spark.createDataFrame(rows, df.schema)
df_subset.show(truncate=False)
```

```
# explode_outer doesn't discard empty arrays
df_exploded = df_exploded.withColumn("query_param", explode_outer(col("clean_query_list")))

df_exploded = df_exploded.withColumn("query_key", split(col("query_param"), "=").getItem(0)).withColumn("query_value", split(col("query_param"), "=").getItem(1))

```

```
df.filter((df["domain"] != "canvas.ieec.mx") & (df["query_param"].isNotNull())).groupBy("domain").count().show(truncate=False)
```



Bastion experiment
Remote
sudo ufw allow 9001/tcp
sudo ufw reload
or
sudo iptables -A INPUT -p tcp --dport 9001 -j ACCEPT

Local
ssh -fN -R <remote_port>:localhost:<local_port> user@bastion

Remote
ssh -J user@bastion -p <remote_port> user@localhost
